<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>No Humans Required</title>
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" type="image/png" href="images/NHR_logo.png">
</head>
<body class="antialiased leading-normal">
    <div class="container-base">
        <header class="text-center mb-10">
            <h1 class="text-4xl text-indigo-700 mt-10">No Humans Required</h1>
            <p class="text-2xl text-gray-700 mt-2">Introducing NHR: A Fully Automated Pipeline for Image Editing Dataset Creation</p>
            <div class="mt-8 flex justify-center flex-wrap button-group">
                <a href="#" class="btn btn-secondary">
                    <img
                        class="w-5 h-5 mr-2"
                        src="https://upload.wikimedia.org/wikipedia/commons/b/bc/ArXiv_logo_2022.svg"
                        alt="arXiv logo"
                        aria-hidden="true"
                    />
                    Paper
                </a>
                <a href="https://huggingface.co/datasets/iitolstykh/NHR-Edit" class="btn btn-secondary">
                <img
                    class="w-5 h-5 mr-2"
                    src="https://huggingface.co/front/assets/huggingface_logo.svg"
                    alt="Hugging Face logo"
                    aria-hidden="true"
                />
                NHR-Edit Dataset
                </a>
                <a href="https://huggingface.co/iitolstykh/Bagel-NHR-Edit" class="btn btn-secondary">
                    <img
                        class="w-5 h-5 mr-2"
                        src="https://huggingface.co/front/assets/huggingface_logo.svg"
                        alt="Hugging Face logo"
                        aria-hidden="true"
                    />
                    Bagel-NHR-Edit
                </a>
                <a href="#" class="btn btn-primary btn-disabled cursor-not-allowed">
                    <svg class="w-5 h-5 mr-2 disabled-logo" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <circle cx="12" cy="12" r="10" fill="#FF6347"/>
                        <path d="M9.75 15.5L15.25 12L9.75 8.5V15.5Z" fill="#FFFFFF"/>
                    </svg>
                    Demo (Not Available Yet)
                </a>
            </div>
        </header>

        <main>
            <section class="section-box">
                <div class="ba-group">

                <div class="ba-caption" id="ba-caption"></div>

                <div class="ba-container">
                    <div class="ba animate" id="ba-viewport">
                    <img src="images/img0_after.jpg"  alt="After"  class="ba__img ba__img--after">
                    <img src="images/img0_before.jpg" alt="Before" class="ba__img ba__img--before">
                    <div class="ba__handle"></div>
                    <input type="range" class="ba__range" min="0" max="100" value="50"
                            aria-label="Compare this image pair">
                    </div>

                    <div class="ba-picker" id="pair-picker"></div>
                </div>
                </div>
            </section>

            <section class="section-box">
                <h2 class="text-indigo-600 font-bold">Abstract</h2>

                <p>We are pleased to introduce <strong>No Humans Required (NHR)</strong>, a novel, fully automated pipeline engineered for the generation of high-fidelity, pixel-perfect image editing sequences. This innovation culminates in <strong>NHR-Edit</strong>, a new dataset meticulously constructed to facilitate the training and rigorous evaluation of advanced image editing models.</p>

                <p>NHR addresses the inherent limitations of traditional image editing datasets, specifically mitigating the biases and inefficiencies associated with manual annotation processes. Our methodology leverages state-of-the-art <strong>Vision-Language Models (VLMs), Text2Image generators and Large Language Models (LLMs)</strong> and other advanced artificial intelligence paradigms with a bunch of euristics. These models are synergistically employed for both the programmatic synthesis of diverse image content and the subsequent stringent filtration of generated data. This automated paradigm ensures the production of an extensive array of complex and varied editing scenarios, thereby advancing the capabilities of AI in visual manipulation tasks.</p>
            </section>

            <section class="section-box">
                <h2 class="text-indigo-600 font-bold">The NHR Pipeline: Unleashing Automated Excellence</h2>
                <p>
                    The NHR-Edit dataset is generated through a sophisticated, 
                    multi-stage pipeline designed for optimal efficiency and quality:
                </p>
                <ol class="list-disc list-inside space-y-2 mt-4 ml-4">
                    <li><strong>Starting Point Generation:</strong> To obtain high-quality input images, we generate them using the latest Flux1.dev model as the SOTA T2I model and OpenAI O3 as one of the best LLMs. This results in high-quality, very detailed, and style-varied images with many details to edit in later steps.</li>
                    <li><strong>Automated Sequence Continuance:</strong> At its core, the NHR pipeline utilizes LLMs to generate intricate editing instructions from initial prompt. These instructions, ranging from subtle adjustments to complex compositional changes, are then executed by advanced image editing models, creating the "edited" versions of the original images. This ensures a rich variety of editing tasks and corresponding ground-truth results.</li>
                    <li><strong>Intelligent Filtration and Quality Control:</strong> A critical component of NHR is its robust filtration system. This stage employs a suite of SOTA models to assess the quality, accuracy, and coherence of the generated image-editing pairs. Models are tasked with verifying that the edits accurately reflect the instructions, ensuring visual fidelity, and flagging any artifacts or inconsistencies. This automated quality control guarantees a dataset free from human error and subjective interpretations.</li>
                    <li><strong>Scalability and Diversity:</strong> By removing human intervention, the NHR pipeline offers unprecedented scalability. It can continuously generate new data, allowing for the creation of vast and diverse datasets tailored to specific research needs. This eliminates the bottleneck of manual annotation, accelerating the development and training of more robust and versatile image editing AI.
                    </li>
                </ol>
                <div class="mt-6 w-full mx-auto bg-blue-100 rounded-xl p-4 shadow-inner">
                    <img src="images/pipeline.jpg" alt="NHR Pipeline" class="w-full h-auto rounded-lg"
                         onerror="this.onerror=null;this.src='https://placehold.co/600x400/BFDBFE/1E40AF?text=Architecture+Diagram';">
                    <p class="text-center text-sm text-gray-500 mt-2">
                        Proposed NoHumansRequired framework scheme.
                    </p>
                </div>
            </section>

            <section class="section-box">
                <h2 class="text-indigo-600 font-bold">Autonomous Dataset Generation Pipeline</h2>
                <p>
                    A significant contribution of this work is the <strong>fully automatic pipeline for dataset generation</strong>,
                    which ensures a scalable and high-quality source of data for training and evaluation. This pipeline
                    involves the following steps:
                </p>
                <ol class="list-decimal list-inside space-y-2 mt-4 ml-4">
                    <li><strong>Prompt Generation with LLM:</strong> A large language model is used to automatically generate diverse and complex input prompts for image editing tasks.</li>
                    <li><strong>Initial Image Generation with Flux:</strong> An initial image is generated based on the LLM-generated prompt using the Flux1.dev model.</li>
                    <li><strong>Image Editing with In-house DiT:</strong> The generated image is then edited by a proprietary model according to specific instructions, demonstrating its instructive editing capabilities.</li>
                    <li><strong>Quality Assessment with Qwen Model:</strong> The edited images are assessed for pixel-perfect accuracy, instruction following, and aesthetics using the Qwen model, ensuring high standards for the dataset.</li>
                    <li><strong>Strong Augmentations:</strong> For obtained triplets, we use inversion and bootstrap composition operations to expand number of available edits.</li>
                    <li><strong>Backward consistency filter:</strong> After multiplying number of samples, we perform filtering, based on inversion or composition quality with filtering origin of augmented triplet to clean all the possible errors.</li>
                </ol>
                <p class="mt-4">
                    This automated approach allows for the creation of a vast and diverse dataset, crucial for
                    developing and continuously improving robust models for Image-to-Image tasks.
                </p>
                <div class="grid grid-cols-1 sm:grid-cols-2 gap-6">
                    <div class="mt-6 w-full mx-auto bg-blue-100 rounded-xl p-4 shadow-inner">
                        <img src="images/01_general_categories.jpg" alt="NHR Pipeline" class="w-full h-auto rounded-lg">
                        <p class="text-center text-sm text-gray-500 mt-2">
                            General category group distribution.
                        </p>
                    </div>

                    <div class="mt-6 w-full mx-auto bg-blue-100 rounded-xl p-4 shadow-inner">
                        <img src="images/02_misc_breakdown.jpg" alt="NHR Pipeline" class="w-full h-auto rounded-lg">
                        <p class="text-center text-sm text-gray-500 mt-2">
                            Miscellaneous operations distribution.
                        </p>
                    </div>

                    <div class="mt-6 w-full mx-auto bg-blue-100 rounded-xl p-4 shadow-inner">
                        <img src="images/03_composite_breakdown_log.jpg" alt="NHR Pipeline" class="w-full h-auto rounded-lg">
                        <p class="text-center text-sm text-gray-500 mt-2">
                            Composite operations distribution (logarithmic scale).
                        </p>
                    </div>

                    <div class="mt-6 w-full mx-auto bg-blue-100 rounded-xl p-4 shadow-inner">
                        <img src="images/04_styles.jpg" alt="NHR Pipeline" class="w-full h-auto rounded-lg">
                        <p class="text-center text-sm text-gray-500 mt-2">
                            Image style distribution; 'standard' stands for images with no explicit style.
                        </p>
                    </div>
                </div>

            </section>

            <section class="section-box">
                <div class="collage-viewer" id="collage-viewer">
                    <div class="collage-picker" id="collage-picker">
                    </div>
                    <div class="collage-display">
                        <img src="" alt="Selected collage" class="collage-image" id="collage-image">
                    </div>
                </div>
            </section>

            <section class="section-box">
                <h2 class="text-indigo-600 font-bold">Publications</h2>
                <p>
                    [1] Layer team, "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining."
                    <em></em>, 2025.
                    <a href="#" class="text-indigo-600 hover:underline ml-2">[Paper]</a>
                </p>
                <h3 class="text-xl font-semibold text-gray-800 mt-6">BibTeX</h3>
                <pre class="bg-gray-100 p-4 rounded-lg text-sm overflow-x-auto"><code>@inproceedings{layer|NoHumansRequired,
  title={NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining.},
  author={Layer team},
  booktitle={},
  year={2025}
}</code></pre>
            </section>
        </main>

        <footer class="text-center text-gray-500 text-sm mt-12 mb-4">
            <p>&copy; 2025 NHR Project. Built with ❤️ and Tailwind CSS.</p>
        </footer>
    </div>
<script src="scripts.js"></script>
</body>
</html>
